# Phase 0: Demo-Augmentation Prompting Baseline

**Goal**: Measure the effect of demo-conditioning in prompts before committing to expensive fine-tuning experiments.

**Total Evaluations**: 240 (20 tasks × 2 models × 2 conditions × 3 trials)

**Budget**: $200-400 API costs

**Timeline**: 1-2 weeks

## Overview

Phase 0 is the first validation gate in the demo-augmentation research roadmap. It tests whether including demonstration trajectories in prompts improves GUI agent performance on Windows Agent Arena (WAA) tasks.

**Research Question**: Does demo-conditioning improve episode success rates in prompted GUI agents?

**Decision Gate**:
- >20pp improvement → Proceed to Phase 1 (fine-tuning infrastructure)
- 10-20pp improvement → Cost-benefit analysis
- <10pp improvement → Publish prompting results, defer fine-tuning

## Background

Previous work (P0 fix validation) showed:
- **Without demo**: 0% success, 6.8 avg steps (random exploration)
- **With demo**: 0% success, 3.0 avg steps (focused actions, better pattern following)

The P0 fix ensured demos persist across ALL steps (not just step 1), fixing the "100% first-action / 0% episode success" problem. Phase 0 extends this to full WAA evaluation.

**Key Innovation**: Demo-conditioning at EVERY step, not just the first action.

## Experimental Design

### Independent Variables

1. **Condition** (2 levels):
   - **Zero-shot**: No demonstration provided
   - **Demo-conditioned**: Demonstration from synthetic demo library

2. **Model** (2 levels):
   - **Claude Sonnet 4.5** (`claude-sonnet-4-5-20250929`)
   - **GPT-4** (`gpt-4`)

### Dependent Variables

- **Episode success rate** (primary): % of tasks completed successfully
- **Steps to completion**: Number of actions taken
- **Cost per task**: API cost in USD

### Tasks

20 diverse WAA tasks spanning all domains:

| Domain | Tasks |
|--------|-------|
| Notepad | notepad_1, notepad_3, notepad_5 |
| Browser | browser_2, browser_4, browser_5, browser_7 |
| Office | office_1, office_4, office_9 |
| File Explorer | file_explorer_1, file_explorer_3, file_explorer_6, file_explorer_9 |
| Settings | settings_1, settings_2, settings_3 |
| Paint | paint_10 |
| Clock | clock_2 |
| Edge | edge_3 |

See `PHASE0_TASKS.txt` for complete list.

### Trials

- **3 trials** per task/model/condition
- Total: 20 tasks × 2 models × 2 conditions × 3 trials = **240 evaluations**

### Demo Source

- Synthetic demonstrations from `demo_library/synthetic_demos/`
- Generated by Claude Sonnet 4.5 using domain knowledge and WAA task specifications
- Format: Step-by-step actions with reasoning and coordinates

## Quick Start

### Prerequisites

1. **Azure VM with WAA server running**:
   ```bash
   uv run python -m openadapt_evals.benchmarks.cli up
   ```

2. **Environment variables set**:
   ```bash
   export ANTHROPIC_API_KEY="your-key"
   export OPENAI_API_KEY="your-key"
   ```

3. **Install dependencies**:
   ```bash
   uv sync
   ```

### Run Tests

Test the infrastructure before starting:

```bash
# Quick smoke test
./scripts/phase0_test.sh --quick

# Full test suite
./scripts/phase0_test.sh

# Integration test (1 real API call)
./scripts/phase0_test.sh --integration
```

### Run Phase 0

```bash
# Full run (240 evaluations, ~12-24 hours)
./scripts/phase0_runner.sh

# Dry run (show plan, no API calls)
./scripts/phase0_runner.sh --dry-run

# Test with single task (2 models × 2 conditions × 3 trials = 12 evals)
./scripts/phase0_runner.sh --tasks notepad_1

# Quick pilot (1 trial per condition, 80 evals)
./scripts/phase0_runner.sh --trials 1

# Resume from checkpoint (after failure)
./scripts/phase0_runner.sh --resume
```

### Monitor Progress

```bash
# Show current status
./scripts/phase0_status.sh

# Watch mode (auto-refresh every 10s)
./scripts/phase0_status.sh --watch

# JSON output
./scripts/phase0_status.sh --json
```

### Analyze Results

```bash
# Run statistical analysis
python scripts/phase0_analyze.py

# Export to JSON
python scripts/phase0_analyze.py --export analysis.json

# Generate plots (requires matplotlib)
python scripts/phase0_analyze.py --plot
```

## Output Structure

```
phase0_results/
├── .checkpoint              # Resume state
├── .cost_log.json          # Cost tracking
├── zero-shot/
│   ├── claude-sonnet-4-5/
│   │   ├── notepad_1_trial1.json
│   │   ├── notepad_1_trial1.log
│   │   ├── notepad_1_trial2.json
│   │   ├── notepad_1_trial2.log
│   │   └── ...
│   └── gpt-4/
│       └── ...
└── demo-conditioned/
    ├── claude-sonnet-4-5/
    │   └── ...
    └── gpt-4/
        └── ...
```

Each result file contains:
```json
{
  "summary": {
    "episode_success": true,
    "total_steps": 5,
    "total_cost": 0.52,
    "final_action": "DONE()"
  },
  "trace": [...],
  "metadata": {
    "task_id": "notepad_1",
    "model": "claude-sonnet-4-5",
    "condition": "demo-conditioned",
    "demo_file": "demo_library/synthetic_demos/notepad_1.txt"
  }
}
```

## Statistical Analysis

The analysis script computes:

### 1. Descriptive Statistics

For each condition:
- Success rate (%)
- Average steps (mean ± std)
- Median steps
- Total cost

### 2. McNemar's Test

Paired statistical test for binary outcomes (success/failure):
- **Null hypothesis**: No difference between conditions
- **p < 0.05**: Significant difference
- Used because same tasks evaluated in both conditions

### 3. Effect Size (Cohen's h)

Measures magnitude of difference:
- **Small**: h = 0.2
- **Medium**: h = 0.5
- **Large**: h = 0.8

### 4. Bootstrap Confidence Intervals

95% CI for difference in success rates:
- 10,000 bootstrap samples
- Shows precision of estimate
- If CI excludes 0, difference is significant

### 5. Failure Mode Analysis

Categorizes tasks that failed in each condition:
- Common failure patterns
- Tasks that benefit most from demos
- Tasks where demos don't help

## Cost Tracking

### Estimated Costs

| Component | Unit Cost | Count | Total |
|-----------|-----------|-------|-------|
| Claude calls | $0.50 | 120 | $60 |
| GPT-4 calls | $1.50 | 120 | $180 |
| Azure VM | $0.20/hr | 100 hrs | $20 |
| **Total** | | | **$260-400** |

### Budget Alerts

The runner tracks costs in real-time:
- **75%**: Warning message
- **90%**: Caution alert
- **100%**: Execution stops

Override with `--budget` flag:
```bash
./scripts/phase0_runner.sh --budget 500
```

### Cost Log Format

```json
{
  "start_time": "2026-01-18T00:00:00Z",
  "budget_limit": 400,
  "total_cost": 245.60,
  "runs_completed": 180,
  "runs": [
    {
      "timestamp": "2026-01-18T01:23:45Z",
      "model": "claude-sonnet-4-5",
      "task": "notepad_1",
      "cost": 0.52,
      "tokens_input": 5432,
      "tokens_output": 987
    }
  ]
}
```

## Error Handling

### Rate Limiting

The runner handles rate limits automatically:
- **Retry**: 3 attempts with exponential backoff
- **Backoff**: 30s, 60s, 90s
- **Sleep**: 5s between successful calls

### Checkpoint/Resume

If the runner is interrupted:
```bash
# Resume from last checkpoint
./scripts/phase0_runner.sh --resume
```

The checkpoint file tracks:
- Completed evaluations
- Failed evaluations (with errors)
- Progress percentage

### Failure Handling

- **Transient failures**: Automatically retried
- **Permanent failures**: Logged and skipped
- **Partial results**: Always saved

## Interpreting Results

### Success Criteria

**Phase 0 Success**:
- >20pp improvement (demo-conditioned vs zero-shot)
- p < 0.05 (statistically significant)
- Effect size h > 0.5 (medium or larger)

**Acceptable Outcome**:
- 10-20pp improvement
- Borderline significance
- Publishable as workshop paper

**Failure**:
- <5pp improvement
- Not significant
- Pivot to failure analysis

### Decision Gates

Based on results, we make one of these decisions:

| Improvement | Decision |
|-------------|----------|
| >20pp | **PROCEED** to Phase 1 (training infrastructure) |
| 10-20pp | **COST-BENEFIT** analysis, possibly workshop paper |
| <10pp | **DEFER** fine-tuning, publish prompting results |

### Example Interpretation

```
COMPARISON: Zero-Shot vs Demo-Conditioned (claude-sonnet-4-5)
===============================================================
Baseline Success Rate:   35.0%
Treatment Success Rate:  58.5%
Improvement:             +23.5 pp
95% CI:                  [+15.2, +31.8] pp
Effect Size (Cohen's h): 0.621
McNemar's p-value:       0.0012
Significant (p<0.05):    YES

Interpretation:
  Effect Size: Large improvement
  Statistical Significance: YES (p=0.0012)

Decision Gate:
  ✓ PROCEED to Phase 1 (>20pp improvement)
```

**Meaning**: Demo-conditioning improves success rate by 23.5 percentage points. This is statistically significant (p=0.0012) and a large effect (h=0.621). Proceed to Phase 1 (fine-tuning infrastructure).

## Next Steps

### If Phase 0 Succeeds (>20pp improvement)

1. **Analyze failure modes**: What tasks benefit most from demos?
2. **Write up findings**: Document prompting baseline results
3. **Plan Phase 1**: Training infrastructure (4-6 weeks)
4. **Budget Phase 2**: Fine-tuning experiments ($2-5k GPU)

### If Phase 0 Shows Marginal Results (10-20pp)

1. **Cost-benefit analysis**: Is 10-20pp worth $5k GPU + 6 months?
2. **Alternative approaches**: Could we improve prompts further?
3. **Publication strategy**: Workshop paper on prompting results

### If Phase 0 Fails (<10pp)

1. **Failure analysis**: Why don't demos help?
2. **Alternative hypotheses**: What's blocking performance?
3. **Pivot strategy**: Architecture changes vs prompting improvements
4. **Publication**: "When Demonstrations Don't Help" (workshop)

## Troubleshooting

### Server Not Reachable

```bash
# Check VM status
az vm show --name waa-eval-vm --resource-group OPENADAPT-AGENTS --show-details

# Start VM and WAA server
uv run python -m openadapt_evals.benchmarks.cli up

# Verify server
uv run python -m openadapt_evals.benchmarks.cli probe --server http://VM_IP:5000
```

### Task File Not Found

```bash
# Verify task file exists
cat PHASE0_TASKS.txt

# Check demo files exist
ls -l demo_library/synthetic_demos/
```

### Out of Budget

```bash
# Check current cost
./scripts/phase0_status.sh

# Increase budget (if approved)
./scripts/phase0_runner.sh --budget 600
```

### Checkpoint Corrupted

```bash
# Remove checkpoint to start fresh
rm phase0_results/.checkpoint

# Re-run (will re-do all evaluations)
./scripts/phase0_runner.sh
```

## Timeline

### Week 1: Setup & Pilot (3-5 days)

- [ ] Test infrastructure (phase0_test.sh)
- [ ] Run pilot (1-2 tasks, 1 trial)
- [ ] Verify results format
- [ ] Fix any issues

### Week 2: Full Evaluation (7-10 days)

- [ ] Run all 240 evaluations
- [ ] Monitor progress daily
- [ ] Handle any failures
- [ ] Track costs

### Week 3: Analysis (2-3 days)

- [ ] Run statistical analysis
- [ ] Generate plots
- [ ] Write up results
- [ ] **DECISION GATE**: Proceed to Phase 1?

**Total**: 2-3 weeks from start to decision

## References

- **Demo-Augmentation Strategy**: `DEMO_AUGMENTATION_STRATEGY.md`
- **P0 Fix Validation**: `DEMO_TEST_SUMMARY.md`
- **Synthetic Demo Library**: `demo_library/synthetic_demos/README.md`
- **WAA Integration**: `openadapt_evals/adapters/waa_live.py`
- **API Agent (with P0 fix)**: `openadapt_evals/agents/api_agent.py`

## Support

For questions or issues:
1. Check this README first
2. Review test output: `./scripts/phase0_test.sh`
3. Check status: `./scripts/phase0_status.sh`
4. Review logs: `phase0_results/*/*.log`

## License

MIT License (same as openadapt-evals)
