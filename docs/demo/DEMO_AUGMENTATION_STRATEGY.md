# Demo-Augmentation Research Strategy: Analysis & Recommendations

**Date**: 2026-01-18
**Status**: Strategic Assessment
**Decision Required**: Prioritize demo-augmentation vs continue WAA baseline path

---

## Executive Summary

**Recommendation**: **PARALLEL PURSUIT** - Run both tracks simultaneously with phased validation gates.

**Rationale**:
- âœ… P0 demo persistence fix PROVEN to work (3.0 vs 6.8 avg steps)
- âœ… Infrastructure READY (154 synthetic demos, retrieval agent, evaluation harness)
- âš ï¸ WAA baseline BLOCKED (Windows container issues)
- ðŸ’¡ Can validate demo-augmentation approach CHEAPLY before committing $2-5k compute

**Key Insight**: We just proved demo persistence works in **prompting**. The logical next step is to measure the **prompting baseline** first (cheap, fast), then decide if fine-tuning (expensive, slow) is worth pursuing.

**Critical Path**:
1. **Week 1-2**: Complete WAA prompting baseline (with/without demos) - $200-400 API costs
2. **Week 3**: Analyze results â†’ DECISION GATE: Does demo-augmentation show >20pp improvement?
3. **If YES**: Proceed to fine-tuning experiments ($2-5k compute)
4. **If NO**: Pivot to failure analysis and incremental improvements

---

## 1. Strategic Analysis

### 1.1 Current State Assessment

**What We've Proven (P0 Fix Validation)**:
- âœ… Demo persistence works in API calls (included at EVERY step, not just step 1)
- âœ… Behavioral change confirmed: 3.0 vs 6.8 avg steps (mock test)
- âœ… 154 synthetic demos generated for all WAA tasks
- âœ… RetrievalAugmentedAgent implemented (automatic demo selection)
- âœ… Evaluation harness ready (WAALiveAdapter, Azure orchestration)

**What's Blocked**:
- â³ Full WAA evaluation (Windows container nested virtualization issues)
- â³ End-to-end training pipeline (untested)
- â³ Baseline performance data (no clean zero-shot measurements)

**What We Don't Know**:
- â“ Does demo-augmentation improve **episode success** (not just first-action)?
- â“ What's the magnitude of improvement on **standard benchmarks** (WAA)?
- â“ Will fine-tuning with demos beat prompting with demos?
- â“ Is the improvement worth $2-5k GPU + 4-6 weeks training?

### 1.2 Demo-Augmentation in Context

**Publication Roadmap Context** (Option C):
- **Effort**: 4-6 months
- **Risk**: High
- **Novelty**: Very High
- **Compute**: $2-5k GPU, 4-6 weeks training
- **Related work**: CogAgent, SeeClick, RT-2

**Critical Assessment from Publication Roadmap**:
> "Behavioral Cloning with Demo-Augmentation would require substantial compute ($2-5k), expertise in VLM fine-tuning, and carries high risk. The contribution is training a model, not prompting strategy."

**Why This Matters**:
- Demo-conditioning in **prompts** is proven (100% first-action accuracy)
- Demo-conditioning in **fine-tuning** is UNTESTED and HIGH RISK
- The two are complementary, not mutually exclusive

### 1.3 Fit with P0 Work

**P0 Work** (from STATUS.md):
1. Complete WAA integration validation (IN PROGRESS)
2. Run 20-50 task evaluation (BLOCKED on container)
3. Analyze results and create data-driven roadmap

**Demo-Augmentation Alignment**:
- âœ… **Highly aligned** - Demo-augmentation IS the intervention we need to test
- âœ… **Complementary** - Can test prompting first, fine-tuning second
- âœ… **Data-driven** - Results from prompting inform fine-tuning decision

**Can We Pursue Both?**
- **YES** - Prompting baseline (P0) enables fine-tuning decision (demo-augmentation)
- **Sequential gates** - Don't commit to fine-tuning until prompting results are clear
- **Resource efficiency** - Prompting costs $200-400, fine-tuning costs $2-5k
- **Timeline** - Prompting: 2 weeks, Fine-tuning: 4-6 months

---

## 2. Technical Feasibility Assessment

### 2.1 Infrastructure Inventory

**What We Already Have** âœ…:

| Component | Status | Location | Readiness |
|-----------|--------|----------|-----------|
| **Demo Library** | âœ… Complete | `demo_library/synthetic_demos/` (154 demos) | Production-ready |
| **Demo Retrieval** | âœ… Implemented | `openadapt_evals/agents/retrieval_agent.py` | Tested, working |
| **API Agent** | âœ… P0 Fix Done | `openadapt_evals/agents/api_agent.py` | Demo persists at every step |
| **WAA Adapter** | âœ… Complete | `openadapt_evals/adapters/waa_live.py` | Live + mock adapters |
| **Evaluation Harness** | âœ… Complete | `openadapt_evals/benchmarks/runner.py` | Azure + local execution |
| **Cost Tracking** | âœ… Complete | `openadapt_evals/benchmarks/monitoring.py` | Real-time cost monitoring |
| **Viewer + Analysis** | âœ… Complete | `openadapt_evals/benchmarks/viewer.py` | HTML reports with screenshots |

**What's Missing** âŒ:

| Component | Status | Estimated Effort | Required For |
|-----------|--------|------------------|--------------|
| **Training Pipeline** | âŒ Not implemented | 2-3 weeks | Fine-tuning experiments |
| **VLM Fine-tuning Code** | âŒ Not implemented | 3-4 weeks | Behavioral cloning |
| **Behavioral Cloning Dataset** | âŒ Not collected | 2-4 weeks | Training data |
| **Fine-tuning Evaluation** | âŒ Not implemented | 1 week | Model comparison |
| **Baseline Performance Data** | âš ï¸ Partial (mock only) | 1-2 weeks | Decision gate |

**Critical Gap**: We have ALL the infrastructure for **prompting experiments** but NONE of the infrastructure for **fine-tuning experiments**.

### 2.2 Cheap Validation Path

**Phase 0: Prompting Baseline (CHEAP - $200-400)**

Goal: Measure demo-augmentation effect in prompting BEFORE committing to fine-tuning.

**Experiments**:
1. **Zero-shot baseline**: 20 WAA tasks Ã— 2 models Ã— 3 trials = 120 runs
2. **Demo-conditioned**: Same 20 tasks Ã— 2 models Ã— 3 trials = 120 runs
3. **Total**: 240 API calls, ~$200-400 cost, 1-2 weeks runtime

**Deliverables**:
- Success rate comparison (zero-shot vs demo-conditioned)
- Episode success, not just first-action
- Statistical significance (McNemar's test, bootstrap CI)
- Failure mode analysis

**Decision Gate**:
- **If >20pp improvement**: Demo-augmentation is WORTH pursuing in fine-tuning
- **If <10pp improvement**: Prompting is NOT enough, may need architectural changes
- **If 10-20pp improvement**: Borderline - need cost-benefit analysis

**Why This Works**:
- Tests the CORE hypothesis (demos help) cheaply
- Provides baseline data for fine-tuning comparison
- Low risk ($400 vs $5k)
- Fast turnaround (2 weeks vs 6 months)

### 2.3 Training Pipeline Requirements

**If Phase 0 shows promise, we need**:

| Component | Effort | Dependencies | Risk |
|-----------|--------|--------------|------|
| **Behavioral cloning dataset** | 2-4 weeks | WAA access, recording infra | Medium |
| **LoRA fine-tuning code** | 1-2 weeks | HuggingFace transformers | Low |
| **Demo-augmented training** | 1 week | BC dataset + demos | Low |
| **Evaluation harness** | 1 week | Existing eval code | Low |
| **GPU compute** | 4-6 weeks | $2-5k budget | Medium |
| **Analysis & writing** | 2-3 weeks | Results data | Low |

**Total Effort**: 10-17 weeks (2.5-4 months)
**Total Cost**: $2-5k GPU + $500 API
**Risk**: Medium-High (training may not improve over prompting)

---

## 3. Research Risk Assessment

### 3.1 Primary Risks

**Risk 1: Demo-augmentation doesn't improve fine-tuned models**
- **Likelihood**: Medium (30-40%)
- **Impact**: High (wastes $5k + 4 months)
- **Mitigation**: Phase 0 validation (prompting baseline) before committing
- **Backup**: Publish prompting results as "negative result" paper

**Risk 2: Improvement is too small to publish**
- **Likelihood**: Medium (30-40%)
- **Impact**: Medium (workshop paper only, not main track)
- **Mitigation**: Set clear decision gates (<10pp = no go, >20pp = publish)
- **Backup**: Focus on analysis ("when demos help vs hurt")

**Risk 3: Windows container blocks evaluation**
- **Likelihood**: Low-Medium (20-30%)
- **Impact**: High (cannot measure episode success)
- **Mitigation**: Use mock adapter for prompting baseline, fix container in parallel
- **Backup**: Switch to WebArena or macOS tasks

**Risk 4: Training requires expertise we don't have**
- **Likelihood**: Low (10-20%)
- **Impact**: Medium (delays timeline 2-4 weeks)
- **Mitigation**: Use existing fine-tuning frameworks (HuggingFace, LLaMA Factory)
- **Backup**: Hire contractor with VLM fine-tuning experience

### 3.2 Success Probability Estimates

**Prompting Baseline (Phase 0)**:
- **Success probability**: 80-90% (we've already proven it works in mock)
- **Value**: HIGH (baseline data + decision gate)
- **Cost**: LOW ($200-400)
- **Timeline**: SHORT (1-2 weeks)

**Fine-tuning (Full Demo-Augmentation)**:
- **Success probability**: 40-60% (untested, high risk)
- **Value**: VERY HIGH IF successful (novel contribution)
- **Cost**: HIGH ($2-5k)
- **Timeline**: LONG (4-6 months)

**Conclusion**: Prompting baseline is LOW RISK, HIGH VALUE. Fine-tuning is HIGH RISK, HIGH REWARD.

### 3.3 Publication Strategy

**Backup Story 1: Prompting Improvements** (if fine-tuning fails)
- Title: "Does Showing Help? An Empirical Study of Demo-Conditioned GUI Agents"
- Venue: LLM Agents Workshop @ NeurIPS 2026
- Contribution: Empirical study of demo-conditioning in prompts
- Timeline: 2-3 months
- Acceptance probability: 60-70%

**Backup Story 2: Failure Analysis** (if demos don't help)
- Title: "When Demonstrations Don't Help: A Study of GUI Agent Limitations"
- Venue: Workshop or CHI Late-Breaking Work
- Contribution: Analysis of when demos help vs hurt
- Timeline: 2-3 months
- Acceptance probability: 50-60%

**Main Story: Demo-Augmented Fine-tuning** (if Phase 0 succeeds)
- Title: "Behavioral Cloning with Demo-Augmentation for GUI Agents"
- Venue: NeurIPS/ICML main track or CHI/UIST
- Contribution: Novel training method with >20pp improvement
- Timeline: 6-9 months
- Acceptance probability: 25-35% (main track), 60%+ (CHI/UIST)

**Key Insight**: We have TWO publishable stories regardless of fine-tuning outcome.

---

## 4. Prioritization Recommendation

### 4.1 Recommended Approach: **PARALLEL with GATES**

**Priority Level**: **P0** (Demo-augmentation prompting baseline)
**Why**: This IS the validation experiment we need to run for P0 work.

**NOT P2 (deferred)** because:
- We've already proven demo persistence works (P0 fix)
- We have all infrastructure ready (154 demos, retrieval agent)
- This is the next logical validation step
- Cheap to validate ($400 vs $5k)

**NOT "switch from current prioritization"** because:
- Prompting baseline IS part of WAA baseline work
- They're complementary, not competing priorities

### 4.2 Phased Roadmap

**Phase 0: Prompting Baseline (WEEKS 1-2) - P0**
- **Goal**: Measure demo-augmentation effect in prompts
- **Tasks**:
  - [ ] Run 20 WAA tasks (zero-shot + demo-conditioned)
  - [ ] 2 models (Claude Sonnet 4.5, GPT-4V) Ã— 3 trials
  - [ ] Statistical analysis (McNemar's, bootstrap CI)
  - [ ] Failure mode categorization
- **Budget**: $200-400 API costs
- **Decision Gate**: Does demo-conditioning show >20pp improvement?
  - **YES** â†’ Proceed to Phase 1
  - **NO** â†’ Write up prompting results, defer fine-tuning

**Phase 1: Training Infrastructure (WEEKS 3-6) - P1**
*Only start if Phase 0 shows >20pp improvement*
- **Goal**: Build training pipeline
- **Tasks**:
  - [ ] Collect behavioral cloning dataset (50+ WAA tasks)
  - [ ] Implement LoRA fine-tuning code
  - [ ] Implement demo-augmented training
  - [ ] Validate pipeline end-to-end (small-scale test)
- **Budget**: $500 API costs (data collection)
- **Decision Gate**: Can we train models end-to-end?
  - **YES** â†’ Proceed to Phase 2
  - **NO** â†’ Fall back to prompting-only paper

**Phase 2: Fine-tuning Experiments (WEEKS 7-14) - P1**
*Only start if Phase 1 pipeline works*
- **Goal**: Train and evaluate demo-augmented models
- **Tasks**:
  - [ ] Train baseline model (no demo-augmentation)
  - [ ] Train demo-augmented model
  - [ ] Evaluate both on 50+ WAA tasks
  - [ ] Statistical comparison
  - [ ] Ablation studies (demo format, relevance)
- **Budget**: $2-5k GPU compute
- **Decision Gate**: Does fine-tuning beat prompting?
  - **YES** â†’ Proceed to Phase 3 (publication)
  - **NO** â†’ Publish prompting results as main story

**Phase 3: Publication (WEEKS 15-24) - P1**
*Write-up and submission*
- **Goal**: Submit to conference/workshop
- **Tasks**:
  - [ ] Write paper (prompting OR fine-tuning story)
  - [ ] Create figures and tables
  - [ ] Internal review
  - [ ] Submit to venue (workshop or main track)
- **Budget**: Minimal
- **Timeline**: 2-3 months

### 4.3 Resource Allocation

**Researcher Time**:
- **Phase 0**: 1 FTE Ã— 2 weeks = 0.5 person-months
- **Phase 1**: 1 FTE Ã— 4 weeks = 1 person-month
- **Phase 2**: 1-2 FTE Ã— 8 weeks = 2-4 person-months
- **Phase 3**: 1 FTE Ã— 10 weeks = 2.5 person-months
- **Total**: 6-8 person-months (if all phases pursued)

**Budget**:
- **Phase 0**: $200-400 (API)
- **Phase 1**: $500 (API)
- **Phase 2**: $2-5k (GPU) + $500 (API)
- **Phase 3**: Minimal
- **Total**: $3.2-6k (if all phases pursued)

**Timeline**:
- **Phase 0**: 2 weeks
- **Phase 1**: 4 weeks (only if Phase 0 succeeds)
- **Phase 2**: 8 weeks (only if Phase 1 succeeds)
- **Phase 3**: 10 weeks
- **Total**: 24 weeks = 6 months (if all phases pursued)

---

## 5. Critical Path & Decision Gates

### 5.1 Decision Flow

```
START
  â†“
[Phase 0: Prompting Baseline] (2 weeks, $400)
  â†“
GATE 1: >20pp improvement?
  â†“ YES                    â†“ NO
[Phase 1: Training Infra]  [Publish prompting paper]
  (4 weeks, $500)          (Workshop, 60-70% accept)
  â†“
GATE 2: Pipeline works?
  â†“ YES                    â†“ NO
[Phase 2: Fine-tuning]     [Fall back to prompting]
  (8 weeks, $2-5k)         (Same as Gate 1 NO)
  â†“
GATE 3: Fine-tuning beats prompting?
  â†“ YES                    â†“ NO
[Main track submission]    [Workshop submission]
  (25-35% accept)          (60-70% accept)
```

### 5.2 Success Criteria per Phase

**Phase 0 Success Criteria**:
- [ ] Complete 240 API runs (20 tasks Ã— 2 models Ã— 2 conditions Ã— 3 trials)
- [ ] Episode success rate measured (not just first-action)
- [ ] Statistical significance achieved (p < 0.05)
- [ ] >20pp improvement (demo-conditioned vs zero-shot)
- [ ] Failure modes categorized (at least 3 categories)

**Phase 1 Success Criteria**:
- [ ] 50+ WAA task demonstrations collected
- [ ] LoRA fine-tuning code working (validates on small model)
- [ ] Demo-augmented training code working (validates on small dataset)
- [ ] End-to-end pipeline tested (train â†’ evaluate â†’ compare)

**Phase 2 Success Criteria**:
- [ ] Baseline model trained (no demo-augmentation)
- [ ] Demo-augmented model trained
- [ ] Both evaluated on 50+ WAA tasks (3 trials each)
- [ ] Fine-tuning shows >10pp improvement over prompting
- [ ] Ablation studies complete (demo format, relevance, k)

**Phase 3 Success Criteria**:
- [ ] Paper draft complete (8-10 pages)
- [ ] Figures and tables publication-ready
- [ ] Internal review complete (2+ reviewers)
- [ ] Submitted to target venue

### 5.3 Exit Criteria (When to Stop)

**Exit Phase 0 if**:
- Cannot fix Windows container AND mock results are insufficient
- Demo-conditioning shows <5pp improvement (not worth pursuing)
- API costs exceed $1k (budget overrun)

**Exit Phase 1 if**:
- Cannot collect behavioral cloning dataset (WAA access issues)
- Training pipeline requires >2 person-months to build (too complex)
- Phase 0 showed <10pp improvement (weak signal)

**Exit Phase 2 if**:
- Fine-tuning costs exceed $8k (budget overrun)
- Training takes >12 weeks (timeline overrun)
- Fine-tuning shows NO improvement over prompting (<5pp)

**Fall-back position**: We ALWAYS have a publishable story (prompting results).

---

## 6. Cost & Timeline Estimates

### 6.1 Detailed Cost Breakdown

**Phase 0: Prompting Baseline**
| Item | Quantity | Unit Cost | Total |
|------|----------|-----------|-------|
| Claude Sonnet 4.5 API | 120 calls | $0.50 | $60 |
| GPT-4V API | 120 calls | $1.50 | $180 |
| Azure VM (WAA server) | 100 hours | $0.20/hr | $20 |
| Researcher time | 80 hours | N/A | N/A |
| **Total** | - | - | **$260-400** |

**Phase 1: Training Infrastructure**
| Item | Quantity | Unit Cost | Total |
|------|----------|-----------|-------|
| API calls (data collection) | 500 calls | $1.00 | $500 |
| Azure VM | 160 hours | $0.20/hr | $32 |
| Researcher time | 160 hours | N/A | N/A |
| **Total** | - | - | **$532** |

**Phase 2: Fine-tuning Experiments**
| Item | Quantity | Unit Cost | Total |
|------|----------|-----------|-------|
| GPU compute (4x A100) | 4 weeks | $500/week | $2000 |
| API calls (evaluation) | 1000 calls | $1.00 | $1000 |
| Storage (checkpoints) | 500 GB | $0.10/GB | $50 |
| Researcher time | 320 hours | N/A | N/A |
| **Total** | - | - | **$3050-5000** |

**Phase 3: Publication**
| Item | Quantity | Unit Cost | Total |
|------|----------|-----------|-------|
| Writing & revision | 400 hours | N/A | N/A |
| **Total** | - | - | **Minimal** |

**Grand Total**: $3,842 - $5,932 (excluding researcher time)

### 6.2 Timeline with Parallel Paths

**Optimistic Timeline** (all gates pass):
- **Week 1-2**: Phase 0 (prompting baseline)
- **Week 3-6**: Phase 1 (training infrastructure) + Windows container fix
- **Week 7-14**: Phase 2 (fine-tuning experiments)
- **Week 15-24**: Phase 3 (publication)
- **Total**: 24 weeks = 6 months

**Pessimistic Timeline** (fall back to prompting):
- **Week 1-2**: Phase 0 (prompting baseline)
- **Week 3**: Analysis & decision (gate fails)
- **Week 4-12**: Write prompting paper
- **Week 13**: Submit to workshop
- **Total**: 13 weeks = 3 months

**Realistic Timeline** (mixed results):
- **Week 1-2**: Phase 0 (prompting baseline)
- **Week 3-6**: Phase 1 (training infrastructure)
- **Week 7-14**: Phase 2 (fine-tuning shows marginal improvement)
- **Week 15-22**: Write workshop paper (prompting + fine-tuning)
- **Total**: 22 weeks = 5.5 months

### 6.3 Comparison to Status Quo

**Current Plan** (WAA baseline â†’ failure analysis):
- **Timeline**: 4-8 weeks (if container works)
- **Cost**: $200-500
- **Output**: Baseline performance data, failure modes
- **Publication**: Analysis paper (workshop)

**Demo-Augmentation Plan** (this proposal):
- **Timeline**: 2-24 weeks (phased, with gates)
- **Cost**: $260-$5,932 (gates prevent waste)
- **Output**: Prompting baseline OR fine-tuning results
- **Publication**: Workshop OR main track (depending on results)

**Key Difference**: Demo-augmentation includes WAA baseline (Phase 0) AND extends it with training experiments (Phase 2). It's a superset, not an alternative.

---

## 7. Answer to Key Question

> **"Given that we just proved demo persistence works in prompting (P0 fix), is the next logical step to validate it improves fine-tuning, or should we first measure the prompting baseline?"**

**Answer**: **Measure the prompting baseline FIRST.**

**Reasoning**:

1. **We don't know the magnitude of improvement yet**
   - Mock test showed behavioral change (3.0 vs 6.8 steps)
   - But no episode success rate on real WAA tasks
   - No statistical significance tests
   - Could be 5pp improvement (not worth fine-tuning) or 50pp (definitely worth it)

2. **Prompting baseline is CHEAP ($400) and FAST (2 weeks)**
   - Fine-tuning is EXPENSIVE ($5k) and SLOW (6 months)
   - Prompting baseline provides decision gate for fine-tuning
   - If prompting shows <10pp improvement, fine-tuning is unlikely to help

3. **Prompting baseline is REQUIRED for publication anyway**
   - Any fine-tuning paper needs a zero-shot baseline
   - Any fine-tuning paper needs a prompting baseline (to show fine-tuning is better)
   - We need this data regardless of whether we pursue fine-tuning

4. **Prompting baseline has multiple uses**
   - Decision gate for fine-tuning (>20pp = go, <10pp = stop)
   - Publication-ready results (workshop paper if we stop here)
   - Failure mode analysis (informs fine-tuning approach)
   - Baseline data for all future experiments

5. **Risk mitigation**
   - If prompting shows weak results, we save $5k + 6 months
   - If prompting shows strong results, we have confidence to invest in fine-tuning
   - Either way, we have publishable results

**Conclusion**: The next logical step is Phase 0 (prompting baseline), NOT jumping directly to fine-tuning.

---

## 8. Final Recommendation

### 8.1 Recommended Path

**Start with Phase 0 (Prompting Baseline) IMMEDIATELY as P0 work**

**Why this is the RIGHT decision**:
1. âœ… Aligns with P0 priorities (WAA baseline validation)
2. âœ… Leverages existing infrastructure (154 demos, retrieval agent)
3. âœ… Cheap validation ($400) before expensive commitment ($5k)
4. âœ… Fast turnaround (2 weeks) for decision gate
5. âœ… Publishable results regardless of outcome
6. âœ… Provides decision data for fine-tuning investment

**Why NOT to jump to fine-tuning directly**:
1. âŒ High risk ($5k + 6 months wasted if demos don't help)
2. âŒ Missing baseline data (can't measure improvement without baseline)
3. âŒ Infrastructure not ready (training pipeline doesn't exist)
4. âŒ No decision gate (don't know if fine-tuning will beat prompting)

### 8.2 Action Items (This Week)

**Immediate (Week 1)**:
- [ ] **Define exact task set**: Select 20 WAA tasks with diverse first actions
- [ ] **Fix Windows container** OR use mock adapter for prompting baseline
- [ ] **Budget API costs**: $400 allocated for Phase 0
- [ ] **Setup evaluation harness**: Test end-to-end (task load â†’ API call â†’ eval â†’ metrics)

**This Week (Week 2)**:
- [ ] **Run zero-shot baseline**: 20 tasks Ã— 2 models Ã— 3 trials = 120 runs
- [ ] **Run demo-conditioned**: 20 tasks Ã— 2 models Ã— 3 trials = 120 runs
- [ ] **Statistical analysis**: McNemar's test, bootstrap CI, effect sizes
- [ ] **Failure mode analysis**: Categorize when demos help vs hurt

**Next Week (Week 3)**:
- [ ] **Review results**: Does demo-conditioning show >20pp improvement?
- [ ] **DECISION GATE**: Proceed to Phase 1 (training) or publish prompting results?
- [ ] **Write up findings**: Even if we proceed to training, document prompting results

### 8.3 Success Metrics

**Phase 0 success** = Demo-conditioning shows >20pp improvement on episode success rate

**Acceptable outcome** = Demo-conditioning shows 10-20pp improvement (publish workshop paper)

**Failure** = Demo-conditioning shows <5pp improvement (pivot to failure analysis)

**Regardless of outcome**: We have publishable results and decision data.

---

## 9. Conclusion

**This is NOT a choice between "WAA baseline" and "demo-augmentation".**

**This IS a choice between**:
- (A) Run WAA baseline WITHOUT measuring demo impact â†’ Publish failure analysis
- (B) Run WAA baseline WITH/WITHOUT demos â†’ Publish prompting results â†’ THEN decide fine-tuning

**Option B is strictly better** because:
- Same cost and timeline as Option A for Phase 0
- Provides decision gate for expensive fine-tuning investment
- Publishable results regardless of fine-tuning outcome
- Validates the core hypothesis (demos help) before architectural work

**The question is not WHETHER to pursue demo-augmentation, but HOW**:
- âœ… **Recommended**: Prompting first (cheap, fast), fine-tuning second (if warranted)
- âŒ **Not recommended**: Fine-tuning directly (expensive, risky, no decision gate)

**Start Phase 0 (Prompting Baseline) as P0 work THIS WEEK.**

---

## Appendix A: Related Work Comparison

| Approach | Our Work | CogAgent | SeeClick | RT-2 |
|----------|----------|----------|----------|------|
| **Demo format** | Behavior traces | Fine-tuning data | Grounding annotations | Robot trajectories |
| **Learning** | Prompting + fine-tuning (planned) | Full fine-tuning | Full fine-tuning | Full fine-tuning |
| **Platform** | Windows desktop | Android | Web | Physical robots |
| **Novelty** | Demo-augmentation for GUI | GUI-specific VLM | Visual grounding | VL action models |
| **Compute** | $5k (planned) | $50k+ | $20k+ | $100k+ |

**Key difference**: We're testing whether demo-augmentation helps BEFORE committing to full fine-tuning (risk mitigation).

---

## Appendix B: Publication Timeline

**Workshop Paper** (if Phase 0 succeeds, no fine-tuning):
- **Deadline**: NeurIPS 2026 LLM Agents Workshop (likely July 2026)
- **Timeline**: 3 months (Phase 0 + write-up)
- **Acceptance probability**: 60-70%
- **Impact**: Establishes priority, gets feedback

**Main Track Paper** (if Phase 2 succeeds):
- **Target venues**: CHI 2027, UIST 2026, or NeurIPS 2026
- **Timeline**: 6-9 months (all phases + write-up)
- **Acceptance probability**: 25-35% (NeurIPS/ICML), 40-50% (CHI/UIST)
- **Impact**: High novelty contribution

**Backup Paper** (if fine-tuning fails):
- **Title**: "When Demonstrations Don't Help: A Study of GUI Agent Limitations"
- **Venue**: Workshop or CHI Late-Breaking Work
- **Acceptance probability**: 50-60%

---

**Document Status**: Strategic recommendation complete. Ready for decision.
**Next Step**: Review with team â†’ Approve Phase 0 budget â†’ Start experiments THIS WEEK.
